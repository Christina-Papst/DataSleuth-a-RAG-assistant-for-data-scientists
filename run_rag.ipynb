{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chatbot for data analysis\n",
    "This chatbot aims to be a help to data scientists and analysts, who need to manage unstructured information. It was written on a laptop without GPU, therefore the chosen models and database were selected to be as lightweight as possible. This affects the performance of the bot accordingly. If you have a more powerful computer, feel free to swap out the models with more potent ones.\n",
    "Please check the answers for their veracity\n",
    "The interface of this chatbot is a Jupyter notebook, in order to make it easier for data scientists to use it in their environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerankers import Reranker\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from rag_utils import create_database, generate_embeddings, store_embeddings, clean_text, create_corpus, store_knowledgebase, chunk_document, query_pinecone, generate_response, clear_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Before you can start your exciting data journey, you need to decide on a few hyperparameters. If you want to adjust them for every question separately, feel free to do so\n",
    "\n",
    "API_key (str): your Pinecone API-key\n",
    "model_name (str): in order to make this bot as lightweight as possible, both the retriever and the generative part use the same model, a T5 one (will be relevant for embedding. If you want to use a different model type, you might need to change the embedding logic)\n",
    "db_name (str): the name you wish to give your database instance\n",
    "top_k (int, optional): The number of top results to return from the Pinecone query.\n",
    "chunk_size (int, optional): The size of each chunk for processing.\n",
    "overlap (int, optional): The overlap between chunks.\n",
    "temperature (float, optional): The temperature parameter to control creativity.\n",
    "max_new_tokens (int, optional): The maximum number of new tokens to generate.\n",
    "bm25_weight (float, optional): The weight for BM25 scores.\n",
    "semantic_search_weight (float, optional): The weight for semantic search scores.\n",
    "ranker: reranker, for more options look into the documentation of the pythons reranker library\n",
    "directory (folder path): path to the folder, where the text files are stored that you want to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"enter your API key here\"\n",
    "model_name = \"google/flan-t5-large\"  #t5-base\n",
    "db_name = \"pine-new\"\n",
    "top_k=10\n",
    "temperature=0.4\n",
    "chunk_size=200\n",
    "overlap=20\n",
    "max_new_tokens=300\n",
    "bm25_weight=0.7\n",
    "semantic_search_weight=0.3\n",
    "ranker = Reranker('mixedbread-ai/mxbai-rerank-large-v1', model_type='cross-encoder')\n",
    "directory = \"name or path of your directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create your knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a text corpus out of your txt files\n",
    "corpus = create_corpus(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pinecone client instance\n",
    "os.environ[\"PINECONE_API_KEY\"] = API_key\n",
    "pc = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the database\n",
    "index_name = \"pine-new\"\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1024, # Replace with your model dimensions # 768  #512 #1024\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "    \n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store your embedded corpus in your database\n",
    "store_knowledgebase(model_name, corpus, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Ask your questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example question\n",
    "query = \"Enter your query here\"\n",
    "prompt = \"Form new, full sentences out of the information.\"\n",
    "response = generate_response(query, prompt, model_name, index, corpus, top_k, chunk_size, overlap, temperature, max_new_tokens, bm25_weight, semantic_search_weight, ranker)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
